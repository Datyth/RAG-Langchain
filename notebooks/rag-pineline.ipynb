{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Ingestion to Vector database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Read documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents(pdf_dir):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_dir)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob('*.pdf'))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_dir}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f'Processing file {pdf_file}')\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            for document in documents:\n",
    "                metadata = document.metadata\n",
    "                metadata['source'] = str(pdf_file)\n",
    "                document.metadata = metadata\n",
    "                all_documents.append(document)\n",
    "            print(f'Loaded {len(documents)} pages')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {pdf_file}: {e}\")\n",
    "    \n",
    "    print(f'Total documents: {len(all_documents)}')\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files in ..\\data\\pdf\n",
      "Processing file ..\\data\\pdf\\attention-is-all-you-need.pdf\n",
      "Loaded 11 pages\n",
      "Processing file ..\\data\\pdf\\Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
      "Loaded 9 pages\n",
      "Processing file ..\\data\\pdf\\Generic Algorithm.pdf\n",
      "Loaded 4 pages\n",
      "Total documents: 24\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = '../data/pdf/'\n",
    "all_documents = process_all_documents(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total splitted documents splitted from 24 documents: 104 chunks\n",
      "First splitted document content:\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents, chunk_size = 1000, chunk_overlap = 100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    splitted_docs = splitter.split_documents(documents)\n",
    "    if splitted_docs:\n",
    "        print(f'Total splitted documents splitted from {len(documents)} documents: {len(splitted_docs)} chunks')\n",
    "        print(f'First splitted document content:\\n{splitted_docs[0].page_content[:50]}\\n')\n",
    "    else:\n",
    "        print('No documents were splitted. Please check the input documents.')\n",
    "    return splitted_docs\n",
    "\n",
    "chunks = split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Embedding all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_PROJECT\\traditional-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', collection_name = 'pdf_documents', persist_directory: str = '../data/vector_store/chroma_db'):\n",
    "        self.model_name = model_name\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.model = self.load_model()\n",
    "        self.client = self.load_db_client()\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name = self.collection_name,\n",
    "            metadata = {\"description\": \"PDF document embeddings for RAG\"}\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            print(f'Loading model {self.model_name}')\n",
    "            model = SentenceTransformer(self.model_name)\n",
    "            print(f'Loading successfully \\nEmbedding dimension: {model.get_sentence_embedding_dimension()}')\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def load_db_client(self):\n",
    "        try:\n",
    "            print(f'Loading ChromaDB client with persist directory')\n",
    "            os.makedirs(self.persist_directory, exist_ok = True)\n",
    "            client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            print('ChromaDB client loaded successfully')\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ChromaDB client: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def embed_texts(self, texts: List[str]):\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model is not loaded.\")\n",
    "        print(f'Embedding for {len(texts)} texts')\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f'Embeddings shape: {embeddings.shape}')\n",
    "        return embeddings\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def query(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_embedding = self.embed_texts([query_text])[0].reshape(1, -1)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding.tolist(),\n",
    "            n_results=top_k\n",
    "        )\n",
    "        return results['documents'][0], results['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model all-MiniLM-L6-v2\n",
      "Loading successfully \n",
      "Embedding dimension: 384\n",
      "Loading ChromaDB client with persist directory\n",
      "ChromaDB client loaded successfully\n"
     ]
    }
   ],
   "source": [
    "embedding_manager = EmbeddingManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 104 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (104, 384)\n",
      "Adding 104 documents to vector store...\n",
      "Successfully added 104 documents to vector store\n",
      "Total documents in collection: 104\n"
     ]
    }
   ],
   "source": [
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.embed_texts(texts)\n",
    "embedding_manager.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, embedding_manager: EmbeddingManager):\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.2) -> List[Tuple[str, Dict[str, Any]]]:\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        try:\n",
    "            query_embedding = self.embedding_manager.embed_texts([query])[0].reshape(1, -1)\n",
    "            results = self.embedding_manager.collection.query(\n",
    "                query_embeddings=query_embedding.tolist(),\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if(results['documents'] and results['metadatas']):\n",
    "                for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):\n",
    "                    doc_embedding = self.embedding_manager.model.encode([doc])[0].reshape(1, -1)\n",
    "                    similarity = cosine_similarity(query_embedding, doc_embedding)[0][0]\n",
    "                    if similarity >= score_threshold:\n",
    "                        retrieved_docs.append((doc, metadata))\n",
    "                        similarity_score = 1 - similarity\n",
    "                        print(f\"Retrieved document with similarity {similarity_score:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Document similarity {similarity:.4f} below threshold\")\n",
    "            else:\n",
    "                print(\"No documents retrieved from the collection.\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is generic algorithm?'\n",
      "Top K: 5, Score threshold: 0.2\n",
      "Embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document with similarity 0.5619\n",
      "Retrieved document with similarity 0.5819\n",
      "Retrieved document with similarity 0.5826\n",
      "Retrieved document with similarity 0.6011\n",
      "Retrieved document with similarity 0.6113\n",
      "Result 1:\n",
      "\n",
      "Content: Genetic Algorithms\n",
      "STEPHANIE FORREST\n",
      "Department of Computer Science, University of New Mexico, Albuquerque ^forrest@cs.unm.edu&\n",
      "A genetic algorithm is a computa-\n",
      "tional model of biological evolution. Ge-\n",
      "netic\n",
      "algorithms\n",
      "are\n",
      "useful\n",
      "both\n",
      "as\n",
      "search methods for solving problems\n",
      "and for modeling evolutionary systems.\n",
      "In genetic algorithms, binary strings\n",
      "are stored in a computer’s memory and\n",
      "over time are modified in much the\n",
      "same way that populations of individu-\n",
      "als evolve under natural selection. Al-\n",
      "though\n",
      "the\n",
      "computational\n",
      "setting\n",
      "is\n",
      "highly simplified when compared with\n",
      "the natural world, genetic algorithms\n",
      "are\n",
      "capable\n",
      "of\n",
      "evolving\n",
      "surprisingly\n",
      "complex\n",
      "and\n",
      "interesting\n",
      "structures.\n",
      "These\n",
      "structures,\n",
      "called\n",
      "individuals,\n",
      "can represent solutions to problems,\n",
      "strategies for playing games, visual im-\n",
      "ages, or computer programs.\n",
      "Genetic algorithms are loosely based\n",
      "on ideas from population genetics. First,\n",
      "a population of individuals is created\n",
      "randomly. In the simplest case, each...\n",
      "Metadata: {'creator': '', 'author': '', 'creationDate': 'D:19960612123822', 'total_pages': 4, 'content_length': 987, 'page': 0, 'modDate': \"D:20251017045614-07'00'\", 'format': 'PDF 1.4', 'creationdate': 'D:19960612123822', 'trapped': '', 'producer': 'Acrobat Distiller 2.0 for Power Macintosh; modified using iText 4.2.0 by 1T3XT', 'file_path': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'subject': 'ACM Comput. Surv. 1996.28:77-80', 'source': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'keywords': '', 'moddate': '2025-10-17T04:56:14-07:00', 'doc_index': 90, 'title': 'Genetic algorithms'}\n",
      "\n",
      "Result 2:\n",
      "\n",
      "Content: (1010100111) zero copies. After selec-\n",
      "tion, the genetic operators are applied\n",
      "probabilistically;\n",
      "the\n",
      "first\n",
      "individual\n",
      "has its first bit mutated from a 0 to a 1,\n",
      "and crossover combines the last two in-\n",
      "dividuals into two new ones. The result-\n",
      "ing population is shown in the box la-\n",
      "beled T(N11).\n",
      "In recent years, “genetic algorithms”\n",
      "have taken many forms, and in some\n",
      "cases bear little resemblance to Hol-\n",
      "land’s original formulation. Researchers\n",
      "have experimented with different types\n",
      "of representations, crossover and muta-\n",
      "tion operators, special-purpose opera-\n",
      "tors, and different approaches to repro-\n",
      "duction\n",
      "and\n",
      "selection.\n",
      "However,\n",
      "all\n",
      "these methods have a family resem-\n",
      "blance in that they take some inspira-\n",
      "tion from biological evolution and from\n",
      "Holland’s\n",
      "original\n",
      "genetic\n",
      "algorithm.\n",
      "Books\n",
      "that\n",
      "describe\n",
      "the\n",
      "theory\n",
      "and\n",
      "practice of genetic algorithms in greater\n",
      "detail include Holland [1975], Goldberg\n",
      "[1989], Davis [1991], Koza [1992], Hol-\n",
      "land et al. [1986], and Mitchell [1996]....\n",
      "Metadata: {'modDate': \"D:20251017045614-07'00'\", 'creationDate': 'D:19960612123822', 'producer': 'Acrobat Distiller 2.0 for Power Macintosh; modified using iText 4.2.0 by 1T3XT', 'author': '', 'doc_index': 94, 'source': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'keywords': '', 'title': 'Genetic algorithms', 'trapped': '', 'subject': 'ACM Comput. Surv. 1996.28:77-80', 'moddate': '2025-10-17T04:56:14-07:00', 'creationdate': 'D:19960612123822', 'format': 'PDF 1.4', 'page': 1, 'creator': '', 'file_path': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'total_pages': 4, 'content_length': 992}\n",
      "\n",
      "Result 3:\n",
      "\n",
      "Content: ters. In some cases, the parameter set-\n",
      "tings that lead to the exact greatest (or\n",
      "least) value of the function are of inter-\n",
      "est. In other cases, the exact optimum is\n",
      "not required, just a near optimum, or\n",
      "even a value that represents a slight\n",
      "improvement\n",
      "over\n",
      "the\n",
      "previous\n",
      "best\n",
      "known value. In these latter cases, ge-\n",
      "netic algorithms are often an appropri-\n",
      "ate method for finding good values. The\n",
      "strength of the genetic algorithm lies in\n",
      "its ability to manipulate many parame-\n",
      "ters simultaneously, and it has been\n",
      "used for hundreds of applications, in-\n",
      "cluding aircraft design, tuning parame-\n",
      "ters for algorithms that detect and track\n",
      "multiple signals in an image, and locat-\n",
      "ing regions of stability in systems of\n",
      "nonlinear difference equations.\n",
      "Although there are many problems for\n",
      "which the genetic algorithm can evolve\n",
      "a\n",
      "good\n",
      "solution\n",
      "in\n",
      "reasonable\n",
      "time,\n",
      "there are also problems for which it is\n",
      "inappropriate (such as those in which it\n",
      "is important to find the exact global...\n",
      "Metadata: {'doc_index': 98, 'title': 'Genetic algorithms', 'page': 2, 'subject': 'ACM Comput. Surv. 1996.28:77-80', 'modDate': \"D:20251017045614-07'00'\", 'source': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'content_length': 979, 'creator': '', 'producer': 'Acrobat Distiller 2.0 for Power Macintosh; modified using iText 4.2.0 by 1T3XT', 'format': 'PDF 1.4', 'author': '', 'keywords': '', 'total_pages': 4, 'creationdate': 'D:19960612123822', 'trapped': '', 'file_path': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'creationDate': 'D:19960612123822', 'moddate': '2025-10-17T04:56:14-07:00'}\n",
      "\n",
      "Result 4:\n",
      "\n",
      "Content: problems such as circuit design and job\n",
      "shop\n",
      "scheduling.\n",
      "Genetic\n",
      "algorithms\n",
      "have also been used to evolve computer\n",
      "programs for specific tasks and to de-\n",
      "sign other computational structures, for\n",
      "example, cellular automata rules and\n",
      "sorting networks. In machine learning,\n",
      "they have been used to design neural\n",
      "networks, to design rules for rule-based\n",
      "systems, and to design and control ro-\n",
      "bots. Genetic algorithms are known pri-\n",
      "marily as a problem-solving method,\n",
      "but they can also be used to study and\n",
      "model evolution in various settings, in-\n",
      "cluding biological (ecology, immunology,\n",
      "and populations genetics), social (such\n",
      "as economics and political systems), and\n",
      "cognitive systems.\n",
      "Perhaps the most common application\n",
      "of genetic algorithms is multiparameter\n",
      "function optimization. Many problems\n",
      "can be formulated as a search for an\n",
      "optimal value, where the value is a com-\n",
      "plicated function of some input parame-\n",
      "ters. In some cases, the parameter set-\n",
      "tings that lead to the exact greatest (or...\n",
      "Metadata: {'producer': 'Acrobat Distiller 2.0 for Power Macintosh; modified using iText 4.2.0 by 1T3XT', 'content_length': 997, 'doc_index': 97, 'subject': 'ACM Comput. Surv. 1996.28:77-80', 'keywords': '', 'total_pages': 4, 'format': 'PDF 1.4', 'creationdate': 'D:19960612123822', 'title': 'Genetic algorithms', 'file_path': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'page': 2, 'author': '', 'creationDate': 'D:19960612123822', 'trapped': '', 'moddate': '2025-10-17T04:56:14-07:00', 'creator': '', 'source': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'modDate': \"D:20251017045614-07'00'\"}\n",
      "\n",
      "Result 5:\n",
      "\n",
      "Content: tied to these specific mechanisms. More\n",
      "generally, biological mechanisms of all\n",
      "kinds are being incorporated into com-\n",
      "putational systems, including embryol-\n",
      "ogy,\n",
      "viruses,\n",
      "parasites,\n",
      "and\n",
      "immune\n",
      "systems. From an algorithmic perspec-\n",
      "tive, genetic algorithms join a broader\n",
      "class of stochastic methods for solving\n",
      "problems. An important area of future\n",
      "research is to understand carefully how\n",
      "these algorithms relate to one another\n",
      "Genetic Algorithms\n",
      "•\n",
      "79\n",
      "ACM Computing Surveys, Vol. 28, No. 1, March 1996...\n",
      "Metadata: {'modDate': \"D:20251017045614-07'00'\", 'format': 'PDF 1.4', 'doc_index': 101, 'trapped': '', 'author': '', 'creationdate': 'D:19960612123822', 'total_pages': 4, 'subject': 'ACM Comput. Surv. 1996.28:77-80', 'creationDate': 'D:19960612123822', 'source': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'content_length': 502, 'creator': '', 'file_path': '..\\\\data\\\\pdf\\\\Generic Algorithm.pdf', 'page': 2, 'title': 'Genetic algorithms', 'moddate': '2025-10-17T04:56:14-07:00', 'producer': 'Acrobat Distiller 2.0 for Power Macintosh; modified using iText 4.2.0 by 1T3XT', 'keywords': ''}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retriever.retrieve(query = \"What is generic algorithm?\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\\n\")\n",
    "    print(f\"Content: {result[0][:1000]}...\\nMetadata: {result[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Enhancing LLM Prompt by RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "# print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GROQ model llama3-8b-8192\n",
      "GROQ model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama3-8b-8192\", api_key: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY is not set. Please set it in your environment variables.\")\n",
    "        try:\n",
    "            print(f'Loading GROQ model {self.model_name}')\n",
    "            model = ChatGroq(\n",
    "                model_name=self.model_name, \n",
    "                api_key=self.api_key,\n",
    "                temperature = 0.1,\n",
    "                max_tokens = 1024)\n",
    "            print('GROQ model loaded successfully')\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading GROQ model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def generate_response(self, question: str, context: str) -> str:\n",
    "        prompt_template = \"\"\"You are an AI assistant helping users find information. Use the following context to answer the question accurately and concisely.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        try: \n",
    "            prompt = PromptTemplate(\n",
    "                input_variables = ['context', 'question'],\n",
    "                template = prompt_template\n",
    "            )\n",
    "\n",
    "            formatted_prompt = prompt.format(context = context, question = question)\n",
    "            messages = [\n",
    "                HumanMessage(content = formatted_prompt)\n",
    "            ]\n",
    "            response = self.model.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise e\n",
    "        \n",
    "groq_llm = GroqLLM(api_key = groq_api_key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemini model: gemini-2.5-flash\n",
      "Gemini model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class GeminiLLM:\n",
    "    def __init__(self, model_name: str = \"gemini-2.5-flash\", api_key: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GOOGLE_API_KEY is not set. Please set it in your environment variables.\")\n",
    "        try:\n",
    "            print(f'Loading Gemini model: {self.model_name}')\n",
    "            model = ChatGoogleGenerativeAI(\n",
    "                model=self.model_name,\n",
    "                google_api_key=self.api_key,\n",
    "                temperature=0.1,\n",
    "                max_output_tokens=1024,\n",
    "            )\n",
    "            print('Gemini model loaded successfully')\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Gemini model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "            \n",
    "    def generate_response(self, question: str, context: str) -> str:\n",
    "        prompt_template = \"\"\"You are an AI assistant helping users find information. Use the following context to answer the question accurately and concisely.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        try: \n",
    "            prompt = PromptTemplate(\n",
    "                input_variables=['context', 'question'],\n",
    "                template=prompt_template\n",
    "            )\n",
    "\n",
    "            formatted_prompt = prompt.format(context=context, question=question)\n",
    "            messages = [\n",
    "                HumanMessage(content=formatted_prompt)\n",
    "            ]\n",
    "            response = self.model.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            raise e\n",
    "        \n",
    "gemini_llm = GeminiLLM(api_key = gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self, retriever: Retriever = None, llm: GeminiLLM = None):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "\n",
    "    def get_input_and_response(self, query: str, top_k: int = 5, score_threshold: float = 0.2) -> str:\n",
    "        retrieved_docs = self.retriever.retrieve(query, top_k, score_threshold)\n",
    "        if not retrieved_docs:\n",
    "            return \"No relevant documents found to answer the query.\"\n",
    "\n",
    "        context = \"\\n\\n\".join([doc[0] for doc in retrieved_docs])\n",
    "        response = self.llm.generate_response(query, context)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(retriever = retriever, llm = gemini_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'How does generic algorithm work?'\n",
      "Top K: 5, Score threshold: 0.2\n",
      "Embedding for 1 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1, 384)\n",
      "Retrieved document with similarity 0.6009\n",
      "Retrieved document with similarity 0.6181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved document with similarity 0.6299\n",
      "Retrieved document with similarity 0.6390\n",
      "Retrieved document with similarity 0.6578\n",
      "================================================================================\n",
      "Chatbot response:\n",
      " A genetic algorithm works by:\n",
      "1.  **Creating a random population:** Initially, a population of individuals (often represented as bit strings) is created randomly. Each individual is a candidate solution to a problem.\n",
      "2.  **Evaluating fitness:** Variations among individuals lead to some being more \"fit\" (i.e., better problem solutions) than others.\n",
      "3.  **Selection:** These differences in fitness are used to bias the selection of a new set of candidate solutions. A new population is created by making copies of more successful individuals and deleting less successful ones.\n",
      "4.  **Genetic operators:** The copies are not exact. During the copy operation, genetic operators are applied probabilistically:\n",
      "    *   **Mutation:** Random bit flips occur.\n",
      "    *   **Crossover:** Corresponding substrings are exchanged between two individuals.\n",
      "    These operations transform the previous set of good individuals into a new one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag.get_input_and_response(query = \"How does generic algorithm work?\")\n",
    "print(\"=\" * 80)\n",
    "print(f'Chatbot response:\\n {result}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
